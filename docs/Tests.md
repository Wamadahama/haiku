Using a dictionary found at: http://www.delphiforfun.org/programs/Syllables.htm

I was able to develop a set of 44,000 words and their known syllables. I then compared this data to the algorithm using the following code

```python
def compare_to_dataset(word, count):
    """Compares the word and output generated by the algorithm to currently existing database"""
    # Search dataset syllables.json for the word
    haiku_dict = helpers.read_json_file("res/syllables.json")

    for entry in haiku_dict:
        if entry["word"].upper() == word.upper():
            if count == entry["count"]:
                return True
            else:
                return False

def compare_full_dictionary(data="tpl"):
    """Compares the full dictionary for testing"""
    haiku_dict = helpers.read_json_file("res/syllables.json")

    true_count = 0
    false_count = 0

    for entry in haiku_dict:
        word = entry["word"].upper()
        if compare_to_dataset(word, count_syllables(word)):
            true_count += 1
        else:
            false_count += 1


    if data == "string":
        return "( (true " + str(true_count) + ") (false " + str(false_count) + ") )"
    else:
        return (true_count, false_count)
```

The output was

### true: 32564
### false: 9879

Meaning that 22% are incorrect, but the words that typically fail are the less common english words 
